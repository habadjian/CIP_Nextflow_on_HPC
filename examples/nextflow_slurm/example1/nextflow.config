params {
  config_profile_description = 'adapting beowulf conf to SDSC expanse'
  config_profile_url = 'https://hpc.nih.gov/apps/nextflow.html'
  max_memory = '224 GB'
  max_cpus = 32
  max_time = '72 h'
  project = 'sds196'
  partition = 'compute'
}



// use a local executor for short jobs and it has to give -c and --mem to make nextflow
// allocate the resource automatically. For this the
// settings below may have to be adapted to the allocation for
// the main nextflow job.


profiles {
    expanselocal {
        process {
            executor = 'local'
            cache = 'lenient'
            maxRetries = 3
            queueSize = 100
            memory = "$SLURM_MEM_PER_NODE"
            cpus = "$SLURM_CPUS_PER_TASK"
       }
    }

    expanse {
        process {
            executor = 'slurm'
            maxRetries = 1
            queue = 'compute'
            queueSize = 200
            pollInterval = '2 min'
            queueStatInterval = '5 min'
            submitRateLimit = '6/1min'
            retry.maxAttempts = 1

            clusterOptions = ' --export=ALL --nodes=1 --ntasks-per-node=1 -t 00:55:00 -A sds196 '

            scratch = '/scratch/$USER/job_$SLURM_JOB_ID'
            
     }
        timeline.enabled = true
        report.enabled = true
        report.overwrite = true
    }
}


