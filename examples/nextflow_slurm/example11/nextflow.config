params {
  config_profile_description = 'adapting beowulf conf to SDSC expanse'
  config_profile_url = 'https://hpc.nih.gov/apps/nextflow.html'
  max_memory = '224 GB'
  max_cpus = 32
  max_time = '72 h'
  project = 'sds196'
  partition = 'compute'
  cluster_user = 'hgrabski'
}



// use a local executor for short jobs and it has to give -c and --mem to make nextflow
// allocate the resource automatically. For this the
// settings below may have to be adapted to the allocation for
// the main nextflow job.


profiles {
    expanselocal {
        process {
            executor = 'local'
            cache = 'lenient'
            maxRetries = 3
            queueSize = 100
            memory = "$SLURM_MEM_PER_NODE"
            cpus = "$SLURM_CPUS_PER_TASK"
       }
    }

    expanse {
        process {
            executor = 'slurm'
            maxRetries = 1
            queue = params.partition
            queueSize = 24
            pollInterval = '2 min'
            queueStatInterval = '5 min'
            submitRateLimit = '6/1min'
            retry.maxAttempts = 1

            clusterOptions = ' --export=ALL --nodes=1 --ntasks-per-node=1 -t 00:55:00 -A $params.project '

            scratch = '/scratch/$USER/job_$SLURM_JOB_ID'
        
        //-- * This makes processes associated with low_cpu label to use only 1 core and 512MB ram
        withLabel:low_cpu{
          cpus = 4
          memory = '8 GB'
          time = '3 h'
          queue = 'shared'
          clusterOptions = " --export=ALL --nodes=1 --ntasks-per-node=3 -t 01:00:00 -A $params.project "
        }


        withLabel:average_cpu{
          cpus = 8
          memory = '16 GB'
          time = '3 h'
          queue = 'shared'
          clusterOptions = " --export=ALL --nodes=1 --ntasks-per-node=4  -t 01:00:00 -A $params.project "
        }


        withLabel:decent_gpu{
          cpus = 4
          memory = '4 GB'
          time = '3 h'
          queue = 'gpu-shared'
          clusterOptions = " --export=ALL --nodes=1  --ntasks-per-node=4  -t 01:00:00 -A $params.project --gpus=1"
        }


        withLabel:enough_cpu{
          cpus = 4
          memory = '8 GB'
          time = '3 h'
          queue = 'shared'
          module = ['cpu/0.15.4','gcc/9.2.0', 'openmpi/3.1.6', 'amber/20']
          clusterOptions = "--nodes=1 --ntasks-per-node=4  -t 01:00:00  -A $params.project "
        }

     }
        timeline.enabled = true
        report.enabled = true
        report.overwrite = true
    }
}


